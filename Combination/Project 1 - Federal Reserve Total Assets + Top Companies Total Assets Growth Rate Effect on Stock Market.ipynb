{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this Project we will be tackling the problem: how can we use total asset allocation of top companies and the fed, to see\n",
    "# how it affects S&P500 prices (the stock market) and how can we predict future balance sheet expansion of the fed?\n",
    "\n",
    "# Import Libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7da1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed Webscrape Attempts Commented Out\n",
    "\n",
    "# berkshire_hathaway = 'https://www.macrotrends.net/stocks/charts/BRK.B/berkshire-hathaway/total-assets'\n",
    "\n",
    "# page = requests.get(berkshire_hathaway, headers=headers)\n",
    "\n",
    "# soupBH = BeautifulSoup(page.content, \"html.parser\")\n",
    "# soupBH2 = BeautifulSoup(soupBH.prettify(), \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "# table = soupBH2.findAll(\"table\", {\"class\":\"historical_data_table table\"})[0]\n",
    "\n",
    "# rows = table.findAll(\"tr\")\n",
    "# print(rows)\n",
    "\n",
    "\n",
    "# with open(\"editors.csv\", \"wt+\", newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     for row in rows:\n",
    "#         csv_row = []\n",
    "#         for cell in row.findAll([\"td\", \"th\"]):\n",
    "#             csv_row.append(cell.get_text())\n",
    "#         writer.writerow(csv_row)\n",
    "        \n",
    "# print(soupBH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Failed Webscrape Attempts Commented Out\n",
    "# # Connect to Website\n",
    "\n",
    "# berkshire_hathaway = 'https://www.macrotrends.net/stocks/charts/BRK.B/berkshire-hathaway/total-assets'\n",
    "\n",
    "# page = requests.get(berkshire_hathaway, headers=headers)\n",
    "\n",
    "# soupBH = BeautifulSoup(page.content, \"html.parser\")\n",
    "# soupBH2 = BeautifulSoup(soupBH.prettify(), \"html.parser\")\n",
    "\n",
    "# print(soupBH2.h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e71257a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Found a total of 6 tables.\n",
      "[+] Saving table-1\n",
      "[+] Saving table-2\n",
      "[+] Saving table-3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2 columns passed, passed data had 4 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 2 columns passed, passed data had 4 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 87\u001b[0m\n\u001b[0;32m     80\u001b[0m         save_as_csv(table_name, headers, rows)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#run each code separately, then save \"Table 2\" which is quarterly report for each company\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#delete Table 1 (we will not be looking at yearly report, only quarterly report)\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m main(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.macrotrends.net/stocks/charts/BRK.B/berkshire-hathaway/total-assets\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[127], line 80\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     78\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Saving \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m save_as_csv(table_name, headers, rows)\n",
      "Cell \u001b[1;32mIn[127], line 62\u001b[0m, in \u001b[0;36msave_as_csv\u001b[1;34m(table_name, headers, rows)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_as_csv\u001b[39m(table_name, headers, rows):\n\u001b[1;32m---> 62\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame(rows, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRK.B Total Assets\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBRK.B\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 746\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    747\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    748\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    749\u001b[0m         data,\n\u001b[0;32m    750\u001b[0m         columns,\n\u001b[0;32m    751\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    752\u001b[0m         dtype,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    755\u001b[0m         arrays,\n\u001b[0;32m    756\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    759\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    760\u001b[0m     )\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 510\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    511\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 2 columns passed, passed data had 4 columns"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#This whole code below is web scraping part to get financial data\n",
    "#Table 2 is what we are looking at which is the quarterly report for the company\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "# US english\n",
    "LANGUAGE = \"en-US,en;q=0.5\"\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Constructs and returns a soup using the HTML content of `url` passed\"\"\"\n",
    "    # initialize a session\n",
    "    session = requests.Session()\n",
    "    # set the User-Agent as a regular browser\n",
    "    session.headers['User-Agent'] = USER_AGENT\n",
    "    # request for english content (optional)\n",
    "    session.headers['Accept-Language'] = LANGUAGE\n",
    "    session.headers['Content-Language'] = LANGUAGE\n",
    "    # make the request\n",
    "    html = session.get(url)\n",
    "    # return the soup\n",
    "    return bs(html.content, \"html.parser\")\n",
    "\n",
    "\n",
    "def get_all_tables(soup):\n",
    "    \"\"\"Extracts and returns all tables in a soup object\"\"\"\n",
    "    return soup.find_all(\"table\")\n",
    "\n",
    "\n",
    "def get_table_headers(table):\n",
    "    \"\"\"Given a table soup, returns all the headers\"\"\"\n",
    "    headers = []\n",
    "    for th in table.find(\"tr\").find_all(\"th\"):\n",
    "        headers.append(th.text.strip())\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_table_rows(table):\n",
    "    \"\"\"Given a table, returns all its rows\"\"\"\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = []\n",
    "        # grab all td tags in this table row\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) == 0:\n",
    "            # if no td tags, search for th tags\n",
    "            # can be found especially in wikipedia tables below the table\n",
    "            ths = tr.find_all(\"th\")\n",
    "            for th in ths:\n",
    "                cells.append(th.text.strip())\n",
    "        else:\n",
    "            # use regular td tags\n",
    "            for td in tds:\n",
    "                cells.append(td.text.strip())\n",
    "        rows.append(cells)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def save_as_csv(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'BRK.B Total Assets']).to_csv(f\"BRK.B{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv(table_name, headers, rows)\n",
    "        \n",
    "\n",
    "\n",
    "#run each code separately, then save \"Table 2\" which is quarterly report for each company\n",
    "#delete Table 1 (we will not be looking at yearly report, only quarterly report)\n",
    "\n",
    "main('https://www.macrotrends.net/stocks/charts/BRK.B/berkshire-hathaway/total-assets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv2(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'BLK Total Assets']).to_csv(f\"BLK{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main2(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv2(table_name, headers, rows)\n",
    "\n",
    "main2('https://www.macrotrends.net/stocks/charts/BLK/blackrock/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv3(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'CS Total Assets']).to_csv(f\"CS{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main3(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv3(table_name, headers, rows)\n",
    "\n",
    "main3('https://www.macrotrends.net/stocks/charts/SCHW/charles-schwab/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a28075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv4(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'UBS Total Assets']).to_csv(f\"UBS{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main4(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv4(table_name, headers, rows)\n",
    "\n",
    "main4('https://www.macrotrends.net/stocks/charts/UBS/ubs-group-ag/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv5(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'STT Total Assets']).to_csv(f\"STT{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main5(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv5(table_name, headers, rows)\n",
    "\n",
    "main5('https://www.macrotrends.net/stocks/charts/STT/state-street/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c870d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv6(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'JPM Total Assets']).to_csv(f\"JPM{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main6(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv6(table_name, headers, rows)\n",
    "\n",
    "main6('https://www.macrotrends.net/stocks/charts/JPM/jpmorgan-chase/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv7(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'BK Total Assets']).to_csv(f\"BK{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main7(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv7(table_name, headers, rows)\n",
    "\n",
    "main7('https://www.macrotrends.net/stocks/charts/BK/bank-of-new-york-mellon/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv8(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'GS Total Assets']).to_csv(f\"GS{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main8(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv8(table_name, headers, rows)\n",
    "\n",
    "main8('https://www.macrotrends.net/stocks/charts/GS/goldman-sachs/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv9(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'BAC Total Assets']).to_csv(f\"BAC{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main9(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv9(table_name, headers, rows)\n",
    "\n",
    "main9('https://www.macrotrends.net/stocks/charts/BAC/bank-of-america/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv10(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'NTRS Total Assets']).to_csv(f\"NTRS{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main10(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv10(table_name, headers, rows)\n",
    "\n",
    "main10('https://www.macrotrends.net/stocks/charts/NTRS/northern-trust/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032be5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv11(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'MS Total Assets']).to_csv(f\"MS{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main11(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv11(table_name, headers, rows)\n",
    "\n",
    "main11('https://www.macrotrends.net/stocks/charts/MS/morgan-stanley/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv12(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'COF Total Assets']).to_csv(f\"COF{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main12(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv12(table_name, headers, rows)\n",
    "\n",
    "main12('https://www.macrotrends.net/stocks/charts/COF/capital-one-financial/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv13(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=['Date', 'MET Total Assets']).to_csv(f\"MET{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main13(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv13(table_name, headers, rows)\n",
    "\n",
    "main13('https://www.macrotrends.net/stocks/charts/MET/metlife/total-assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7ce1f12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Found a total of 1 tables.\n",
      "[+] Saving table-1\n"
     ]
    }
   ],
   "source": [
    "#S&P Prices\n",
    "\n",
    "def save_as_csv14(table_name, headers, rows):\n",
    "    pd.DataFrame(rows).to_csv(f\"SP500{table_name}.csv\", index=False)\n",
    "\n",
    "\n",
    "def main14(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_all_tables(soup)\n",
    "    print(f\"[+] Found a total of {len(tables)} tables.\")\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        print(f\"[+] Saving {table_name}\")\n",
    "        save_as_csv14(table_name, headers, rows)\n",
    "\n",
    "main14('https://www.multpl.com/s-p-500-historical-prices/table/by-month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7bfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
